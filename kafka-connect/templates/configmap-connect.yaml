apiVersion: v1

kind: ConfigMap
metadata:
  name: {{ include "kafka-connect.fullname" . }}-connect
  labels:
    app.kubernetes.io/name: {{ include "kafka-connect.name" . }}
    helm.sh/chart: {{ include "kafka-connect.chart" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
data:
  connect.sh: |-
    #!/bin/bash
    # set -e
    sink_prefix={{ .Values.sinkName }}
    sink_path={{ .Values.sinkPath }}
    topic_list_values={{ .Values.topicList }}
    IFS=',' read -a topic_list  <<<  $topic_list_values
    echo Topic list: ${topic_list[@]}

    for my_topic in ${topic_list[@]}
      do
        sink_name="${my_topic}-${sink_prefix}"
        sink_file=${sink_path}/${sink_name}-$(date +%Y-%m).sink

        test -f ${sink_file} || touch ${sink_file}
        test -w ${sink_file} || { echo "File ${sink_file} is not writable" ; exit ; }

        echo "Topic: $my_topic  Sink file: ${sink_file}"

        # Create config file for each topic
        config_name="/etc/filesinkConfig/${sink_name}.properties"

        echo "name=${sink_name}" > $config_name
        echo "connector.class=FileStreamSink" >> $config_name
        echo "tasks.max=1" >> $config_name
        echo "file=${sink_file}" >> $config_name
        echo "topics=$my_topic" >> $config_name

        # Create list of dynamic configs to pass to the connect process
        config_list+=($config_name)
      done

    timeout -s HUP $RESTART_TIMEOUT /opt/bitnami/kafka/bin/connect-standalone.sh /etc/connectConfig/connect-standalone.properties ${config_list[@]}

    # exit code 124 indicates that timeout timed out
    exit_code=$?
    [ $exit_code = "124" ] || { echo "Kafka connect error: $exit_code" ; exit ; }

    # Compress old files
    old_files=$(find ${sink_path} -name "*${sink_prefix}*.sink" -type f -mtime +30)
    for file in $old_files
      do
        test -f $file || { echo "File $file does not exist" ; exit ; }
        archived_file=$file-"$(date +%s)"
        echo "Compressing: $file"
        mv $file $archived_file
        gzip $archived_file
      done

    # backup if enabled
    if [ $BACKUPS_ENABLED = "true" ]; then
      echo "Backing up connect data..."
      bash /etc/backupConfig/backup.sh
      echo "Backup completed."
    fi

  restore.sh: |-

    # sh /opt/bitnami/kafka/filesink/restore.sh
    # restore_topic=connect1
    # restore_files=/opt/bitnami/kafka/filesink/snapshot/connect*.sink
    # kafka_server=kafka
    # restore_from_time="2021-01-19T15:57"
    params=$#
    if [ "$params" -ne 4 ]; then
        echo "Illegal number of parameters: required: 4, supplied: $params"
        echo
        echo "Usage: $0 restore_topic kafka_server \"restore_files\"  \"restore_from_time\""
        echo
        echo "restore_topic: destination topic to restore to"
        echo "kafka_server: kafka server"
        echo "restore_files: path to the files to restore from, example \"/opt/bitnami/kafka/filesink/connect*.sink\" - quoted string"
        echo "restore_from_time: timestamp to restore from, example \"2021-01-19T15:57\" - quoted string"
        echo
        echo "Example:"
        echo
        echo "sh /etc/connectConfig/restore.sh connect1 kafka \"/opt/bitnami/kafka/filesink/snapshot/connect*.sink\"  \"2021-01-19T15:57\""

        exit
    fi

    restore_topic=$1
    kafka_server=$2
    restore_files=$3
    restore_from_time=$4

    printf "\033c"

    echo "Restoring topic: $restore_topic"
    echo "Kafka server: $kafka_server"
    echo
    echo "Restoring from: $restore_files"
    ls $restore_files
    echo
    echo "Restoring from timestamp: $restore_from_time"
    echo

    #check if the files are readable
    echo "The following files will be used for restore:"
    echo
    for file in $restore_files
      do
        test -f $file  || { echo "File $file does not exist" ; exit ; }
        echo $file
      done

    restore_source="/tmp/restoredata.$(date +%s)"
    touch $restore_source || { echo "Cant touch file: $restore_source" ; exit ; }
    test -w $restore_source || { echo "File $restore_source does not exist" ; exit ; }


    cat $restore_files | awk -F'utcTime":"|"}}' '$2 >= "'$restore_from_time'"' > $restore_source

    read -p "Continue (yes/no)?" choice
    case "$choice" in
      yes )
        echo "Executing restore..."
        kafka-console-producer.sh --broker-list $kafka_server:9092 --topic $restore_topic < $restore_source
        ;;
      * )
        echo "Exiting."
        exit
        ;;
    esac



  connect-standalone.properties: |-
    bootstrap.servers={{ .Values.kafkaSvcName }}.{{ .Values.kafkaSvcNamespace }}.svc.cluster.local:9092
    key.converter=org.apache.kafka.connect.storage.StringConverter
    value.converter=org.apache.kafka.connect.storage.StringConverter
    key.converter.schemas.enable=false
    value.converter.schemas.enable=false
    offset.storage.file.filename=/tmp/connect.offsets
    offset.flush.interval.ms=10000
  # connect-file-sink.properties: |-
  #   name={{ .Values.sinkName }}
  #   connector.class=FileStreamSink
  #   tasks.max=1
  #   file=/opt/bitnami/kafka/filesink/kafka.sink
  #   topics={{ .Values.topic }}
  connect-log4j.properties: |-
    log4j.rootLogger={{ .Values.loglevel }}, stdout, connectAppender
    log4j.appender.stdout=org.apache.log4j.ConsoleAppender
    log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
    log4j.appender.connectAppender=org.apache.log4j.DailyRollingFileAppender
    log4j.appender.connectAppender.DatePattern='.'yyyy-MM-dd-HH
    log4j.appender.connectAppender.File=${kafka.logs.dir}/connect.log
    log4j.appender.connectAppender.layout=org.apache.log4j.PatternLayout
    connect.log.pattern=[%d] %p %m (%c:%L)%n
